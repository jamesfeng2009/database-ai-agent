"""åŸºäº OpenAI API çš„ SQL åˆ†ææ™ºèƒ½ä½“"""

import json
import logging
import sys
import time
from typing import Any, Dict, List, Optional

try:
    import httpx
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None
    httpx = None

from .analyzer_base import BaseSQLAnalyzer, get_default_system_message
from .config import OpenAIConfig, OllamaConfig
from .models import (
    OptimizationSuggestion,
    PerformanceIssue,
    SQLAnalysisRequest,
    SQLAnalysisResponse,
)
from .tools import (
    calculate_performance_score,
    detect_performance_issues,
    format_analysis_request,
    generate_optimization_suggestions,
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class StreamingDisplay:
    """æµå¼æ˜¾ç¤ºå·¥å…·ç±»ï¼Œç”¨äºä¼˜åŒ–AIæµå¼å“åº”çš„æ˜¾ç¤ºæ•ˆæœ."""
    
    def __init__(self, max_line_length: int = 80, show_progress: bool = True) -> None:
        """åˆå§‹åŒ–æµå¼æ˜¾ç¤ºå·¥å…·.
        
        Args:
            max_line_length: æ¯è¡Œæœ€å¤§æ˜¾ç¤ºå­—ç¬¦æ•°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
        """
        self.max_line_length = max_line_length
        self.show_progress = show_progress
        self.display_buffer = ""
        self.line_length = 0
        self.char_count = 0
        self.start_time = time.time()
        
    def start(self, message: str = "ğŸ¤– æ­£åœ¨ç”ŸæˆSQLåˆ†æç»“æœ...") -> None:
        """å¼€å§‹æµå¼æ˜¾ç¤º.
        
        Args:
            message: å¼€å§‹æ˜¾ç¤ºçš„æ¶ˆæ¯
        """
        print(message)
        print("=" * 60)
        self.start_time = time.time()
        
    def add_token(self, token: str) -> None:
        """æ·»åŠ æ–°çš„tokenåˆ°æ˜¾ç¤ºç¼“å†²åŒº.
        
        Args:
            token: æ–°çš„tokenå­—ç¬¦ä¸²
        """
        self.char_count += len(token)
        self.display_buffer += token
        self.line_length += len(token)
        
        # å¦‚æœé‡åˆ°æ¢è¡Œç¬¦ï¼Œç«‹å³æ¢è¡Œæ˜¾ç¤º
        if '\n' in token:
            self._handle_newline()
        # å¦‚æœè¡Œå¤ªé•¿ï¼Œæ™ºèƒ½æ¢è¡Œ
        elif self.line_length > self.max_line_length:
            self._handle_line_break()
        else:
            # åœ¨åŒä¸€è¡Œæ›´æ–°æ˜¾ç¤º
            self._update_current_line()
            
    def _handle_newline(self) -> None:
        """å¤„ç†æ¢è¡Œç¬¦."""
        lines = self.display_buffer.split('\n')
        for i, display_line in enumerate(lines[:-1]):
            # æ¸…ç†å½“å‰è¡Œå¹¶æ˜¾ç¤ºå®Œæ•´å†…å®¹
            self._clear_line()
            print(display_line.strip())
        
        # ä¿ç•™æœ€åä¸€è¡Œä½œä¸ºæ–°çš„ç¼“å†²åŒº
        self.display_buffer = lines[-1] if lines else ""
        self.line_length = len(self.display_buffer)
        
        # å¦‚æœæœ€åä¸€è¡Œæœ‰å†…å®¹ï¼Œæ˜¾ç¤ºå®ƒ
        if self.display_buffer.strip():
            self._update_current_line()
            
    def _handle_line_break(self) -> None:
        """å¤„ç†è¡Œå¤ªé•¿çš„æƒ…å†µï¼Œæ™ºèƒ½æ¢è¡Œ."""
        # åœ¨åˆé€‚çš„ä½ç½®æ¢è¡Œï¼ˆä¼˜å…ˆåœ¨ç©ºæ ¼å¤„ï¼‰
        break_point = self.max_line_length
        for i in range(self.max_line_length - 1, max(0, self.max_line_length - 20), -1):
            if i < len(self.display_buffer) and self.display_buffer[i] == ' ':
                break_point = i + 1
                break
        
        # æ˜¾ç¤ºå½“å‰è¡Œå¹¶æ¢è¡Œ
        current_line = self.display_buffer[:break_point].strip()
        self._clear_line()
        print(current_line)
        
        # å‰©ä½™å†…å®¹ä½œä¸ºæ–°è¡Œçš„å¼€å§‹
        self.display_buffer = self.display_buffer[break_point:]
        self.line_length = len(self.display_buffer)
        
        # æ˜¾ç¤ºæ–°è¡Œå¼€å§‹çš„å†…å®¹
        if self.display_buffer.strip():
            self._update_current_line()
            
    def _update_current_line(self) -> None:
        """æ›´æ–°å½“å‰è¡Œæ˜¾ç¤º."""
        if self.show_progress:
            # è®¡ç®—æ˜¾ç¤ºæ—¶é—´
            elapsed = time.time() - self.start_time
            status = f" [{self.char_count}å­—ç¬¦ {elapsed:.1f}s]"
            available_width = self.max_line_length - len(status)
            
            if len(self.display_buffer) <= available_width:
                print(f"\r{self.display_buffer}{status}", end="", flush=True)
            else:
                # æˆªæ–­æ˜¾ç¤ºå¹¶åŠ çœç•¥å·
                truncated = self.display_buffer[:available_width-3] + "..."
                print(f"\r{truncated}{status}", end="", flush=True)
        else:
            print(f"\r{self.display_buffer}", end="", flush=True)
            
    def _clear_line(self) -> None:
        """æ¸…ç†å½“å‰è¡Œ."""
        print(f"\r{' ' * self.max_line_length}\r", end="")
        
    def finish(self) -> None:
        """å®Œæˆæµå¼æ˜¾ç¤º."""
        # ç¡®ä¿æœ€åçš„å†…å®¹è¢«å®Œæ•´æ˜¾ç¤º
        if self.display_buffer.strip():
            self._clear_line()
            print(self.display_buffer.strip())
        else:
            print()  # ç¡®ä¿å…‰æ ‡åœ¨æ–°è¡Œ
            
        # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
        elapsed = time.time() - self.start_time
        print("=" * 60)
        print(f"âœ… åˆ†æå®Œæˆ (å…±ç”Ÿæˆ {self.char_count} ä¸ªå­—ç¬¦ï¼Œè€—æ—¶ {elapsed:.1f}s)")
        print()
        
    def error_cleanup(self) -> None:
        """é”™è¯¯æ—¶çš„æ¸…ç†å·¥ä½œ."""
        if self.display_buffer:
            self._clear_line()
            print()


class SQLAnalyzerAgent(BaseSQLAnalyzer):
    """SQL æ€§èƒ½åˆ†ææ™ºèƒ½ä½“.
    
    åŸºäº OpenAI API çš„æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äºåˆ†æ MySQL æ…¢ SQL æŸ¥è¯¢çš„æ€§èƒ½é—®é¢˜
    å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
    """
    
    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: Optional[str] = None,
        name: str = "sql_analyzer",
        system_message: Optional[str] = None,
        timeout: float = 60.0,
        max_retries: int = 3,
        stream: bool = False,
        show_streaming_progress: bool = True,
    ) -> None:
        """åˆå§‹åŒ– SQL åˆ†ææ™ºèƒ½ä½“.
        
        Args:
            api_key: OpenAI API å¯†é’¥
            model: æ¨¡å‹åç§°
            base_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
            name: æ™ºèƒ½ä½“åç§°
            system_message: ç³»ç»Ÿæç¤ºæ¶ˆæ¯ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤æ¶ˆæ¯
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            show_streaming_progress: æ˜¯å¦æ˜¾ç¤ºæµå¼å“åº”è¿›åº¦
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… openai åŒ…: pip install openai")
        
        super().__init__(model, name, system_message, timeout)
        
        self.api_key = api_key
        self.base_url = base_url
        self.max_retries = max_retries
        self.stream = stream
        self.show_streaming_progress = show_streaming_progress
        
        print(f"ğŸ¤–ï¸ åˆå§‹åŒ– SQL åˆ†ææ™ºèƒ½ä½“ : ä½¿ç”¨çš„æ¨¡å‹ä¸º{self.model}")
        if self.stream:
            progress_status = "å¯ç”¨" if self.show_streaming_progress else "ç¦ç”¨"
            print(f"âš¡ æµå¼å“åº”: å¯ç”¨ (è¿›åº¦æ˜¾ç¤º: {progress_status})")
        
        # åˆ›å»º OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=httpx.Timeout(timeout),
            max_retries=max_retries,
        )
    
    @classmethod
    def from_config(cls, config: OpenAIConfig, name: str = "sql_analyzer") -> "SQLAnalyzerAgent":
        """ä»é…ç½®åˆ›å»ºæ™ºèƒ½ä½“.
        
        Args:
            config: OpenAI é…ç½®
            name: æ™ºèƒ½ä½“åç§°
            
        Returns:
            é…ç½®å¥½çš„ SQL åˆ†ææ™ºèƒ½ä½“
        """
        return cls(
            api_key=config.api_key,
            model=config.model,
            base_url=config.base_url,
            name=name,
            timeout=config.timeout,
            max_retries=config.max_retries,
            stream=config.stream,
            show_streaming_progress=config.show_streaming_progress,
        )
    
    def _get_default_system_message(self) -> str:
        """è·å–é»˜è®¤çš„ç³»ç»Ÿæç¤ºæ¶ˆæ¯.
        
        Returns:
            ç³»ç»Ÿæç¤ºæ¶ˆæ¯å­—ç¬¦ä¸²
        """
        return get_default_system_message()
    
    async def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯• API è¿æ¥.
        
        Returns:
            åŒ…å«è¿æ¥æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        result = {
            "success": False,
            "error": None,
            "details": {},
        }
        
        try:
            logger.info(f"å¼€å§‹æµ‹è¯• API è¿æ¥: {self.base_url}")
            
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": "æµ‹è¯•è¿æ¥"}
                ],
                max_tokens=10,
                temperature=0.1
            )
            
            result["success"] = True
            result["details"] = {
                "model": self.model,
                "base_url": self.base_url,
                "response_id": response.id if hasattr(response, 'id') else None,
                "usage": response.usage.dict() if hasattr(response, 'usage') and response.usage else None,
            }
            logger.info("API è¿æ¥æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            result["error"] = str(e)
            result["details"] = {
                "model": self.model,
                "base_url": self.base_url,
                "error_type": type(e).__name__,
            }
            logger.error(f"API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            
            # æä¾›å…·ä½“çš„é”™è¯¯è¯Šæ–­
            if "Connection" in str(e) or "timeout" in str(e).lower():
                result["diagnosis"] = "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®"
            elif "401" in str(e) or "Unauthorized" in str(e):
                result["diagnosis"] = "API å¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥ API å¯†é’¥æ˜¯å¦æ­£ç¡®"
            elif "404" in str(e) or "Not Found" in str(e):
                result["diagnosis"] = "API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ base_url æ˜¯å¦æ­£ç¡®"
            elif "403" in str(e) or "Forbidden" in str(e):
                result["diagnosis"] = "API è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥ API å¯†é’¥æƒé™"
            elif "429" in str(e) or "Rate limit" in str(e):
                result["diagnosis"] = "API è°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
            elif "model" in str(e).lower():
                result["diagnosis"] = f"æ¨¡å‹ '{self.model}' ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°"
            else:
                result["diagnosis"] = "æœªçŸ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ API é…ç½®"
        
        return result

    async def _handle_streaming_response(self, model: str, messages: List[Dict[str, str]]) -> str:
        """å¤„ç† OpenAI API æµå¼å“åº”.
        
        Args:
            model: æ¨¡å‹åç§°
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            å®Œæ•´çš„å“åº”æ–‡æœ¬
        """
        full_response = ""
        display = StreamingDisplay(show_progress=self.show_streaming_progress)
        
        try:
            display.start()
            
            # ä½¿ç”¨ OpenAI æµå¼ API
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.1,
                max_tokens=2000,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    display.add_token(token)
            
            display.finish()
            
        except Exception as e:
            display.error_cleanup()
            logger.error(f"æµå¼å“åº”å¤„ç†å¤±è´¥: {e}")
            raise
        
        return full_response

    async def analyze_sql(self, request: SQLAnalysisRequest) -> SQLAnalysisResponse:
        """æ™ºèƒ½ä½“åˆ†æ SQL æ€§èƒ½çš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            request: SQL åˆ†æè¯·æ±‚
            
        Returns:
            SQL åˆ†æå“åº”
        """
        # é¦–å…ˆä½¿ç”¨å·¥å…·å‡½æ•°è¿›è¡ŒåŸºç¡€åˆ†æ
        issues = detect_performance_issues(request)
        suggestions = generate_optimization_suggestions(request, issues)
        score = calculate_performance_score(request, issues)
        
        # å‡†å¤‡ç»™ AI çš„è¾“å…¥
        formatted_request = format_analysis_request(request)
        
        # æ„é€ ç»™ AI çš„æ¶ˆæ¯
        user_message = f"""è¯·åˆ†æä»¥ä¸‹ SQL æŸ¥è¯¢çš„æ€§èƒ½ï¼š

        {formatted_request}
        
        è¯·æä¾›ï¼š
        1. è¯¦ç»†çš„æ€§èƒ½åˆ†æ
        2. æ‰§è¡Œè®¡åˆ’è§£è¯»
        3. å‘ç°çš„é—®é¢˜æ€»ç»“
        4. ä¼˜åŒ–å»ºè®®çš„è¯¦ç»†è¯´æ˜
        
        åŸºç¡€åˆ†æç»“æœï¼š
        - æ€§èƒ½è¯„åˆ†ï¼š{score}/100
        - å‘ç°é—®é¢˜æ•°é‡ï¼š{len(issues)}
        - ä¼˜åŒ–å»ºè®®æ•°é‡ï¼š{len(suggestions)}
        """
        
        # ä½¿ç”¨ OpenAI API è¿›è¡Œæ·±åº¦åˆ†æ
        detailed_analysis = ""
        try:
            logger.info(f"å¼€å§‹è°ƒç”¨ AI API: model={self.model}")
            
            if self.stream:
                # æµå¼å“åº”å¤„ç†
                detailed_analysis = await self._handle_streaming_response(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self.system_message},
                        {"role": "user", "content": user_message}
                    ]
                )
            else:
                # éæµå¼å“åº”å¤„ç†
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self.system_message},
                        {"role": "user", "content": user_message}
                    ],
                    temperature=0.1,
                    max_tokens=2000
                )
                
                detailed_analysis = response.choices[0].message.content or "æ— æ³•è·å–è¯¦ç»†åˆ†æ"
            
            logger.info("AI API è°ƒç”¨æˆåŠŸ")
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"AI API è°ƒç”¨å¤±è´¥: {error_msg}")
            
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè¯Šæ–­
            diagnosis = ""
            if "Connection" in error_msg or "timeout" in error_msg.lower():
                diagnosis = "ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
            elif "401" in error_msg:
                diagnosis = "API å¯†é’¥è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®"
            elif "404" in error_msg:
                diagnosis = f"API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ base_url: {self.base_url}"
            elif "403" in error_msg:
                diagnosis = "API è®¿é—®æƒé™ä¸è¶³"
            elif "429" in error_msg:
                diagnosis = "API è°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•"
            elif "model" in error_msg.lower():
                diagnosis = f"æ¨¡å‹ '{self.model}' ä¸å¯ç”¨"
            else:
                diagnosis = f"æœªçŸ¥ API é”™è¯¯: {type(e).__name__}"
            
            detailed_analysis = f"""AI åˆ†æå¤±è´¥: {error_msg}

é”™è¯¯è¯Šæ–­: {diagnosis}

é…ç½®ä¿¡æ¯:
- æ¨¡å‹: {self.model}
- API ç«¯ç‚¹: {self.base_url}
- è¶…æ—¶è®¾ç½®: {self.timeout}ç§’

å»ºè®®:
1. éªŒè¯ API å¯†é’¥å’Œç«¯ç‚¹é…ç½®
2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®
3. å¦‚æœæ˜¯ DeepSeek APIï¼Œç¡®ä¿ base_url ä¸º https://api.deepseek.com

ä½¿ç”¨åŸºç¡€åˆ†æç»“æœã€‚"""
        
        # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’åˆ†æ
        execution_plan_analysis = self._generate_execution_plan_analysis(request)
        
        # ç”Ÿæˆæ€»ç»“
        summary = self._generate_summary(issues, suggestions, score)
        
        return SQLAnalysisResponse(
            summary=summary,
            performance_score=score,
            issues=issues,
            suggestions=suggestions,
            detailed_analysis=detailed_analysis,
            execution_plan_analysis=execution_plan_analysis,
            explain_results=request.explain_results
        )
    
    def _generate_execution_plan_analysis(self, request: SQLAnalysisRequest) -> str:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’åˆ†æ.
        
        Args:
            request: SQL åˆ†æè¯·æ±‚
            
        Returns:
            æ‰§è¡Œè®¡åˆ’åˆ†ææ–‡æœ¬
        """
        if not request.explain_results:
            return "æ— æ‰§è¡Œè®¡åˆ’ä¿¡æ¯"
        
        # è·å–æ•°æ®åº“é€‚é…å™¨
        from .database.adapters import DatabaseAdapterFactory
        from .tools import _detect_database_type
        
        database_type = _detect_database_type(request.explain_results)
        adapter = DatabaseAdapterFactory.create_adapter(database_type)
        
        lines = ["æ‰§è¡Œè®¡åˆ’åˆ†æï¼š"]
        
        total_rows = 0
        has_full_scan = False
        has_index_usage = False
        
        for i, result in enumerate(request.explain_results, 1):
            # ä½¿ç”¨é€‚é…å™¨è·å–æ•°æ®
            table_name = adapter.get_table_name(result)
            connection_type = adapter.get_connection_type(result)
            rows = adapter.get_scan_rows(result)
            index_info = adapter.get_index_info(result)
            extra_info = adapter.get_extra_info(result)
            cost_info = adapter.get_cost_info(result)
            
            lines.append(f"\næ­¥éª¤ {i}: è®¿é—®è¡¨ {table_name}")
            
            # åˆ†æè¿æ¥ç±»å‹
            if adapter.is_full_table_scan(result):
                lines.append("  âš ï¸  å…¨è¡¨æ‰«æ - æ€§èƒ½é£é™©é«˜")
                has_full_scan = True
            elif adapter.is_index_scan(result):
                lines.append(f"  âœ… ä½¿ç”¨ç´¢å¼• ({connection_type})")
                has_index_usage = True
            elif "Bitmap" in connection_type:
                lines.append(f"  ğŸ”„ ä½å›¾æ‰«æ ({connection_type})")
                has_index_usage = True
            
            # åˆ†æè¡Œæ•°ä¿¡æ¯
            if rows:
                lines.append(f"  ğŸ“Š é¢„ä¼°æ‰«æè¡Œæ•°: {rows:,}")
                total_rows += rows
            
            # åˆ†æå®é™…è¡Œæ•°ï¼ˆPostgreSQLç‰¹æœ‰ï¼‰
            actual_rows = cost_info.get("actual_rows")
            if actual_rows:
                lines.append(f"  ğŸ“ˆ å®é™…æ‰«æè¡Œæ•°: {actual_rows:,}")
            
            # åˆ†æç´¢å¼•ä¿¡æ¯
            key = index_info.get("key")
            possible_keys = index_info.get("possible_keys")
            if key:
                lines.append(f"  ğŸ”‘ ä½¿ç”¨ç´¢å¼•: {key}")
            elif possible_keys:
                lines.append(f"  âš ï¸  æœªä½¿ç”¨å¯ç”¨ç´¢å¼•: {possible_keys}")
            
            # åˆ†ææˆæœ¬ä¿¡æ¯ï¼ˆPostgreSQLç‰¹æœ‰ï¼‰
            startup_cost = cost_info.get("startup_cost")
            total_cost = cost_info.get("total_cost")
            if startup_cost:
                lines.append(f"  ğŸ’° å¯åŠ¨æˆæœ¬: {startup_cost}")
            if total_cost:
                lines.append(f"  ğŸ’° æ€»æˆæœ¬: {total_cost}")
            
            # åˆ†æé¢å¤–ä¿¡æ¯
            if extra_info:
                if "Using temporary" in extra_info:
                    lines.append("  âš ï¸  ä½¿ç”¨ä¸´æ—¶è¡¨")
                if "Using filesort" in extra_info:
                    lines.append("  âš ï¸  ä½¿ç”¨æ–‡ä»¶æ’åº")
                if "Using index" in extra_info:
                    lines.append("  âœ… ä½¿ç”¨è¦†ç›–ç´¢å¼•")
                if "Parallel" in extra_info:
                    lines.append("  ğŸ”„ å¹¶è¡Œæ‰§è¡Œ")
                if "Buffers" in extra_info:
                    lines.append("  ğŸ“¦ ç¼“å­˜ä½¿ç”¨")
        
        lines.append(f"\næ€»è®¡é¢„ä¼°æ‰«æè¡Œæ•°: {total_rows:,}")
        
        if has_full_scan:
            lines.append("âš ï¸  æŸ¥è¯¢åŒ…å«å…¨è¡¨æ‰«æï¼Œå»ºè®®ä¼˜åŒ–")
        elif has_index_usage:
            lines.append("âœ… æŸ¥è¯¢æœ‰æ•ˆåˆ©ç”¨äº†ç´¢å¼•")
        
        return "\n".join(lines)
    
    def _generate_summary(
        self, 
        issues: List[PerformanceIssue], 
        suggestions: List[OptimizationSuggestion], 
        score: int
    ) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“.
        
        Args:
            issues: å‘ç°çš„é—®é¢˜
            suggestions: ä¼˜åŒ–å»ºè®®
            score: æ€§èƒ½å¾—åˆ†
            
        Returns:
            æ€»ç»“æ–‡æœ¬
        """
        if score >= 80:
            summary = f"æŸ¥è¯¢æ€§èƒ½è‰¯å¥½ï¼ˆå¾—åˆ†: {score}/100ï¼‰ã€‚"
        elif score >= 60:
            summary = f"æŸ¥è¯¢æ€§èƒ½å°šå¯ï¼ˆå¾—åˆ†: {score}/100ï¼‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´ã€‚"
        else:
            summary = f"æŸ¥è¯¢æ€§èƒ½è¾ƒå·®ï¼ˆå¾—åˆ†: {score}/100ï¼‰ï¼Œæ€¥éœ€ä¼˜åŒ–ã€‚"
        
        if issues:
            critical_issues = [i for i in issues if i.severity == "critical"]
            high_issues = [i for i in issues if i.severity == "high"]
            
            if critical_issues:
                summary += f" å‘ç° {len(critical_issues)} ä¸ªä¸¥é‡é—®é¢˜ã€‚"
            elif high_issues:
                summary += f" å‘ç° {len(high_issues)} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜ã€‚"
            else:
                summary += f" å‘ç° {len(issues)} ä¸ªä¸€èˆ¬é—®é¢˜ã€‚"
        
        if suggestions:
            high_priority = [s for s in suggestions if s.priority == "high"]
            if high_priority:
                summary += f" æä¾›äº† {len(high_priority)} ä¸ªé«˜ä¼˜å…ˆçº§ä¼˜åŒ–å»ºè®®ã€‚"
            else:
                summary += f" æä¾›äº† {len(suggestions)} ä¸ªä¼˜åŒ–å»ºè®®ã€‚"
        
        return summary


class OllamaAgent(BaseSQLAnalyzer):
    """åŸºäº Ollama API çš„ SQL åˆ†ææ™ºèƒ½ä½“.
    
    ä¸“é—¨ç”¨äºä¸æœ¬åœ°éƒ¨ç½²çš„ Ollama æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œåˆ†æ MySQL æ…¢ SQL æŸ¥è¯¢çš„æ€§èƒ½é—®é¢˜
    å¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚
    """
    
    def __init__(
        self,
        model: str,
        base_url: str = "http://localhost:11434",
        name: str = "ollama_sql_analyzer",
        system_message: Optional[str] = None,
        timeout: float = 60.0,
        stream: bool = False,
        show_streaming_progress: bool = True,
    ) -> None:
        """åˆå§‹åŒ– Ollama SQL åˆ†ææ™ºèƒ½ä½“.
        
        Args:
            model: Ollama æ¨¡å‹åç§°
            base_url: Ollama API åŸºç¡€ URL
            name: æ™ºèƒ½ä½“åç§°
            system_message: ç³»ç»Ÿæç¤ºæ¶ˆæ¯ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤æ¶ˆæ¯
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            show_streaming_progress: æ˜¯å¦æ˜¾ç¤ºæµå¼å“åº”è¿›åº¦
        """
        if httpx is None:
            raise ImportError("éœ€è¦å®‰è£… httpx åŒ…: pip install httpx")
        
        super().__init__(model, name, system_message, timeout)
        
        self.base_url = base_url.rstrip('/')
        self.stream = stream
        self.show_streaming_progress = show_streaming_progress
        
        print(f"ğŸ¦™ åˆå§‹åŒ– Ollama SQL åˆ†ææ™ºèƒ½ä½“: ä½¿ç”¨çš„æ¨¡å‹ä¸º {self.model}")
        print(f"ğŸ“¡ APIåœ°å€: {self.base_url}")
        if self.stream:
            progress_status = "å¯ç”¨" if self.show_streaming_progress else "ç¦ç”¨"
            print(f"âš¡ æµå¼å“åº”: å¯ç”¨ (è¿›åº¦æ˜¾ç¤º: {progress_status})")
    
    @classmethod
    def from_config(cls, config: OllamaConfig, name: str = "ollama_sql_analyzer") -> "OllamaAgent":
        """ä»é…ç½®åˆ›å»ºæ™ºèƒ½ä½“.
        
        Args:
            config: Ollama é…ç½®
            name: æ™ºèƒ½ä½“åç§°
            
        Returns:
            é…ç½®å¥½çš„ Ollama SQL åˆ†ææ™ºèƒ½ä½“
        """
        return cls(
            model=config.model,
            base_url=config.base_url,
            name=name,
            timeout=config.timeout,
            stream=config.stream,
            show_streaming_progress=config.show_streaming_progress,
        )
    
    def _get_default_system_message(self) -> str:
        """è·å–é»˜è®¤çš„ç³»ç»Ÿæç¤ºæ¶ˆæ¯.
        
        Returns:
            ç³»ç»Ÿæç¤ºæ¶ˆæ¯å­—ç¬¦ä¸²
        """
        return get_default_system_message()
    
    async def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯• Ollama API è¿æ¥.
        
        Returns:
            åŒ…å«è¿æ¥æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        result = {
            "success": False,
            "error": None,
            "details": {},
        }
        
        try:
            logger.info(f"å¼€å§‹æµ‹è¯• Ollama API è¿æ¥: {self.base_url}")
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": "æµ‹è¯•è¿æ¥",
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    response_data = response.json()
                    result["success"] = True
                    result["details"] = {
                        "model": self.model,
                        "base_url": self.base_url,
                        "response_preview": response_data.get("response", "")[:100],
                        "done": response_data.get("done", False),
                    }
                    logger.info("Ollama API è¿æ¥æµ‹è¯•æˆåŠŸ")
                else:
                    result["error"] = f"HTTP {response.status_code}: {response.text}"
                    result["details"] = {
                        "model": self.model,
                        "base_url": self.base_url,
                        "status_code": response.status_code,
                    }
                    logger.error(f"Ollama API è¿æ¥æµ‹è¯•å¤±è´¥: {result['error']}")
                    
        except Exception as e:
            result["error"] = str(e)
            result["details"] = {
                "model": self.model,
                "base_url": self.base_url,
                "error_type": type(e).__name__,
            }
            logger.error(f"Ollama API è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            
            # æä¾›å…·ä½“çš„é”™è¯¯è¯Šæ–­
            if "Connection" in str(e) or "timeout" in str(e).lower():
                result["diagnosis"] = "Ollama æœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œå’Œç½‘ç»œè¿æ¥"
            elif "404" in str(e) or "Not Found" in str(e):
                result["diagnosis"] = f"Ollama API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ base_url: {self.base_url}"
            elif "TimeoutException" in str(e):
                result["diagnosis"] = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡å“åº”é€Ÿåº¦æˆ–å¢åŠ è¶…æ—¶æ—¶é—´"
            elif "model" in str(e).lower():
                result["diagnosis"] = f"æ¨¡å‹ '{self.model}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æˆ–å…ˆä¸‹è½½æ¨¡å‹"
            else:
                result["diagnosis"] = "æœªçŸ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€å’Œé…ç½®"
        
        return result
    
    async def analyze_sql(self, request: SQLAnalysisRequest) -> SQLAnalysisResponse:
        """ä½¿ç”¨ Ollama åˆ†æ SQL æ€§èƒ½çš„æ ¸å¿ƒæ–¹æ³•.
        
        Args:
            request: SQL åˆ†æè¯·æ±‚
            
        Returns:
            SQL åˆ†æå“åº”
        """
        # é¦–å…ˆä½¿ç”¨å·¥å…·å‡½æ•°è¿›è¡ŒåŸºç¡€åˆ†æ
        issues = detect_performance_issues(request)
        suggestions = generate_optimization_suggestions(request, issues)
        score = calculate_performance_score(request, issues)
        
        # å‡†å¤‡ç»™ AI çš„è¾“å…¥
        formatted_request = format_analysis_request(request)
        
        # æ„é€ ç»™ AI çš„æ¶ˆæ¯
        user_message = f"""è¯·åˆ†æä»¥ä¸‹ SQL æŸ¥è¯¢çš„æ€§èƒ½ï¼š

{formatted_request}

è¯·æä¾›ï¼š
1. è¯¦ç»†çš„æ€§èƒ½åˆ†æ
2. æ‰§è¡Œè®¡åˆ’è§£è¯»
3. å‘ç°çš„é—®é¢˜æ€»ç»“
4. ä¼˜åŒ–å»ºè®®çš„è¯¦ç»†è¯´æ˜

åŸºç¡€åˆ†æç»“æœï¼š
- æ€§èƒ½è¯„åˆ†ï¼š{score}/100
- å‘ç°é—®é¢˜æ•°é‡ï¼š{len(issues)}
- ä¼˜åŒ–å»ºè®®æ•°é‡ï¼š{len(suggestions)}
"""
        
        # æ„é€ å®Œæ•´çš„æç¤ºè¯
        full_prompt = f"{self.system_message}\n\n{user_message}"
        
        # ä½¿ç”¨ Ollama API è¿›è¡Œæ·±åº¦åˆ†æ
        detailed_analysis = ""
        try:
            logger.info(f"å¼€å§‹è°ƒç”¨ Ollama API: model={self.model}")
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                if self.stream:
                    # æµå¼å“åº”å¤„ç†
                    detailed_analysis = await self._handle_streaming_response(client, full_prompt)
                else:
                    # éæµå¼å“åº”å¤„ç†
                    response = await client.post(
                        f"{self.base_url}/api/generate",
                        json={
                            "model": self.model,
                            "prompt": full_prompt,
                            "stream": False
                        }
                    )
                    
                    if response.status_code == 200:
                        response_data = response.json()
                        detailed_analysis = response_data.get("response", "æ— æ³•è·å–è¯¦ç»†åˆ†æ")
                        logger.info("Ollama API è°ƒç”¨æˆåŠŸ")
                    else:
                        raise Exception(f"HTTP {response.status_code}: {response.text}")
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Ollama API è°ƒç”¨å¤±è´¥: {error_msg}")
            
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè¯Šæ–­
            diagnosis = ""
            if "Connection" in error_msg or "timeout" in error_msg.lower():
                diagnosis = "Ollama æœåŠ¡è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ"
            elif "404" in error_msg:
                diagnosis = f"Ollama API ç«¯ç‚¹ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ base_url: {self.base_url}"
            elif "TimeoutException" in error_msg:
                diagnosis = "è¯·æ±‚è¶…æ—¶ï¼Œå»ºè®®å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥æ¨¡å‹å“åº”é€Ÿåº¦"
            elif "model" in error_msg.lower():
                diagnosis = f"æ¨¡å‹ '{self.model}' ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½"
            else:
                diagnosis = f"æœªçŸ¥ Ollama API é”™è¯¯: {type(e).__name__}"
            
            detailed_analysis = f"""AI åˆ†æå¤±è´¥: {error_msg}

é”™è¯¯è¯Šæ–­: {diagnosis}

é…ç½®ä¿¡æ¯:
- æ¨¡å‹: {self.model}
- API ç«¯ç‚¹: {self.base_url}
- è¶…æ—¶è®¾ç½®: {self.timeout}ç§’

å»ºè®®:
1. ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)
2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ (ollama pull {self.model})
3. éªŒè¯ API ç«¯ç‚¹åœ°å€æ˜¯å¦æ­£ç¡®
4. æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®

ä½¿ç”¨åŸºç¡€åˆ†æç»“æœã€‚"""
        
        # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’åˆ†æ
        execution_plan_analysis = self._generate_execution_plan_analysis(request)
        
        # ç”Ÿæˆæ€»ç»“
        summary = self._generate_summary(issues, suggestions, score)
        
        return SQLAnalysisResponse(
            summary=summary,
            performance_score=score,
            issues=issues,
            suggestions=suggestions,
            detailed_analysis=detailed_analysis,
            execution_plan_analysis=execution_plan_analysis,
            explain_results=request.explain_results
        )
    
    async def _handle_streaming_response(self, client: httpx.AsyncClient, prompt: str) -> str:
        """å¤„ç† Ollama æµå¼å“åº”.
        
        Args:
            client: httpx å®¢æˆ·ç«¯
            prompt: å®Œæ•´çš„æç¤ºè¯
            
        Returns:
            å®Œæ•´çš„å“åº”æ–‡æœ¬
        """
        full_response = ""
        display = StreamingDisplay(show_progress=self.show_streaming_progress)
        
        try:
            display.start()
            
            async with client.stream(
                "POST",
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": True
                }
            ) as response:
                if response.status_code == 200:
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                chunk = json.loads(line)
                                if "response" in chunk:
                                    token = chunk["response"]
                                    full_response += token
                                    display.add_token(token)
                                
                                if chunk.get("done", False):
                                    break
                            except json.JSONDecodeError:
                                continue
                    
                    display.finish()
                    
                else:
                    raise Exception(f"HTTP {response.status_code}: {await response.aread()}")
                    
        except Exception as e:
            display.error_cleanup()
            logger.error(f"æµå¼å“åº”å¤„ç†å¤±è´¥: {e}")
            raise
        
        return full_response
    
    def _generate_execution_plan_analysis(self, request: SQLAnalysisRequest) -> str:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’åˆ†æ.
        
        Args:
            request: SQL åˆ†æè¯·æ±‚
            
        Returns:
            æ‰§è¡Œè®¡åˆ’åˆ†ææ–‡æœ¬
        """
        if not request.explain_results:
            return "æ— æ‰§è¡Œè®¡åˆ’ä¿¡æ¯"
        
        # è·å–æ•°æ®åº“é€‚é…å™¨
        from .database.adapters import DatabaseAdapterFactory
        from .tools import _detect_database_type
        
        database_type = _detect_database_type(request.explain_results)
        adapter = DatabaseAdapterFactory.create_adapter(database_type)
        
        lines = ["æ‰§è¡Œè®¡åˆ’åˆ†æï¼š"]
        
        total_rows = 0
        has_full_scan = False
        has_index_usage = False
        
        for i, result in enumerate(request.explain_results, 1):
            # ä½¿ç”¨é€‚é…å™¨è·å–æ•°æ®
            table_name = adapter.get_table_name(result)
            connection_type = adapter.get_connection_type(result)
            rows = adapter.get_scan_rows(result)
            index_info = adapter.get_index_info(result)
            extra_info = adapter.get_extra_info(result)
            cost_info = adapter.get_cost_info(result)
            
            lines.append(f"\næ­¥éª¤ {i}: è®¿é—®è¡¨ {table_name}")
            
            # åˆ†æè¿æ¥ç±»å‹
            if adapter.is_full_table_scan(result):
                lines.append("  âš ï¸  å…¨è¡¨æ‰«æ - æ€§èƒ½é£é™©é«˜")
                has_full_scan = True
            elif adapter.is_index_scan(result):
                lines.append(f"  âœ… ä½¿ç”¨ç´¢å¼• ({connection_type})")
                has_index_usage = True
            elif "Bitmap" in connection_type:
                lines.append(f"  ğŸ”„ ä½å›¾æ‰«æ ({connection_type})")
                has_index_usage = True
            
            # åˆ†æè¡Œæ•°ä¿¡æ¯
            if rows:
                lines.append(f"  ğŸ“Š é¢„ä¼°æ‰«æè¡Œæ•°: {rows:,}")
                total_rows += rows
            
            # åˆ†æå®é™…è¡Œæ•°ï¼ˆPostgreSQLç‰¹æœ‰ï¼‰
            actual_rows = cost_info.get("actual_rows")
            if actual_rows:
                lines.append(f"  ğŸ“ˆ å®é™…æ‰«æè¡Œæ•°: {actual_rows:,}")
            
            # åˆ†æç´¢å¼•ä¿¡æ¯
            key = index_info.get("key")
            possible_keys = index_info.get("possible_keys")
            if key:
                lines.append(f"  ğŸ”‘ ä½¿ç”¨ç´¢å¼•: {key}")
            elif possible_keys:
                lines.append(f"  âš ï¸  æœªä½¿ç”¨å¯ç”¨ç´¢å¼•: {possible_keys}")
            
            # åˆ†ææˆæœ¬ä¿¡æ¯ï¼ˆPostgreSQLç‰¹æœ‰ï¼‰
            startup_cost = cost_info.get("startup_cost")
            total_cost = cost_info.get("total_cost")
            if startup_cost:
                lines.append(f"  ğŸ’° å¯åŠ¨æˆæœ¬: {startup_cost}")
            if total_cost:
                lines.append(f"  ğŸ’° æ€»æˆæœ¬: {total_cost}")
            
            # åˆ†æé¢å¤–ä¿¡æ¯
            if extra_info:
                if "Using temporary" in extra_info:
                    lines.append("  âš ï¸  ä½¿ç”¨ä¸´æ—¶è¡¨")
                if "Using filesort" in extra_info:
                    lines.append("  âš ï¸  ä½¿ç”¨æ–‡ä»¶æ’åº")
                if "Using index" in extra_info:
                    lines.append("  âœ… ä½¿ç”¨è¦†ç›–ç´¢å¼•")
                if "Parallel" in extra_info:
                    lines.append("  ğŸ”„ å¹¶è¡Œæ‰§è¡Œ")
                if "Buffers" in extra_info:
                    lines.append("  ğŸ“¦ ç¼“å­˜ä½¿ç”¨")
        
        lines.append(f"\næ€»è®¡é¢„ä¼°æ‰«æè¡Œæ•°: {total_rows:,}")
        
        if has_full_scan:
            lines.append("âš ï¸  æŸ¥è¯¢åŒ…å«å…¨è¡¨æ‰«æï¼Œå»ºè®®ä¼˜åŒ–")
        elif has_index_usage:
            lines.append("âœ… æŸ¥è¯¢æœ‰æ•ˆåˆ©ç”¨äº†ç´¢å¼•")
        
        return "\n".join(lines)
    
    def _generate_summary(
        self, 
        issues: List[PerformanceIssue], 
        suggestions: List[OptimizationSuggestion], 
        score: int
    ) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“.
        
        Args:
            issues: å‘ç°çš„é—®é¢˜
            suggestions: ä¼˜åŒ–å»ºè®®
            score: æ€§èƒ½å¾—åˆ†
            
        Returns:
            æ€»ç»“æ–‡æœ¬
        """
        if score >= 80:
            summary = f"æŸ¥è¯¢æ€§èƒ½è‰¯å¥½ï¼ˆå¾—åˆ†: {score}/100ï¼‰ã€‚"
        elif score >= 60:
            summary = f"æŸ¥è¯¢æ€§èƒ½å°šå¯ï¼ˆå¾—åˆ†: {score}/100ï¼‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´ã€‚"
        else:
            summary = f"æŸ¥è¯¢æ€§èƒ½è¾ƒå·®ï¼ˆå¾—åˆ†: {score}/100ï¼‰ï¼Œæ€¥éœ€ä¼˜åŒ–ã€‚"
        
        if issues:
            critical_issues = [i for i in issues if i.severity == "critical"]
            high_issues = [i for i in issues if i.severity == "high"]
            
            if critical_issues:
                summary += f" å‘ç° {len(critical_issues)} ä¸ªä¸¥é‡é—®é¢˜ã€‚"
            elif high_issues:
                summary += f" å‘ç° {len(high_issues)} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜ã€‚"
            else:
                summary += f" å‘ç° {len(issues)} ä¸ªä¸€èˆ¬é—®é¢˜ã€‚"
        
        if suggestions:
            high_priority = [s for s in suggestions if s.priority == "high"]
            if high_priority:
                summary += f" æä¾›äº† {len(high_priority)} ä¸ªé«˜ä¼˜å…ˆçº§ä¼˜åŒ–å»ºè®®ã€‚"
            else:
                summary += f" æä¾›äº† {len(suggestions)} ä¸ªä¼˜åŒ–å»ºè®®ã€‚"
        
        return summary


# ä¿æŒå‘åå…¼å®¹çš„ä¾¿åˆ©å‡½æ•°
def create_sql_analyzer_agent(
    api_key: str,
    model: str = "deepseek-chat",
    base_url: Optional[str] = None,
    timeout: float = 60.0,
    max_retries: int = 3,
    stream: bool = False,
    show_streaming_progress: bool = True,
) -> SQLAnalyzerAgent:
    """åˆ›å»º SQL åˆ†ææ™ºèƒ½ä½“çš„ä¾¿åˆ©å‡½æ•°.
    
    Args:
        api_key: OpenAI API å¯†é’¥
        model: æ¨¡å‹åç§°
        base_url: API åŸºç¡€ URLï¼ˆå¯é€‰ï¼‰
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
        show_streaming_progress: æ˜¯å¦æ˜¾ç¤ºæµå¼å“åº”è¿›åº¦
        
    Returns:
        é…ç½®å¥½çš„ SQL åˆ†ææ™ºèƒ½ä½“
    """
    return SQLAnalyzerAgent(
        api_key=api_key,
        model=model,
        base_url=base_url,
        timeout=timeout,
        max_retries=max_retries,
        stream=stream,
        show_streaming_progress=show_streaming_progress,
    )


def create_ollama_agent(
    model: str,
    base_url: str = "http://localhost:11434",
    name: str = "ollama_sql_analyzer",
    timeout: float = 60.0,
    stream: bool = False,
    show_streaming_progress: bool = True,
) -> OllamaAgent:
    """åˆ›å»º Ollama SQL åˆ†ææ™ºèƒ½ä½“çš„ä¾¿åˆ©å‡½æ•°.
    
    Args:
        model: Ollama æ¨¡å‹åç§°
        base_url: Ollama API åŸºç¡€ URL
        name: æ™ºèƒ½ä½“åç§°
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
        show_streaming_progress: æ˜¯å¦æ˜¾ç¤ºæµå¼å“åº”è¿›åº¦
        
    Returns:
        é…ç½®å¥½çš„ Ollama SQL åˆ†ææ™ºèƒ½ä½“
    """
    return OllamaAgent(
        model=model,
        base_url=base_url,
        name=name,
        timeout=timeout,
        stream=stream,
        show_streaming_progress=show_streaming_progress,
    )


# æ–°çš„å·¥å‚å‡½æ•°ï¼Œæ”¯æŒä»é…ç½®åˆ›å»ºæ™ºèƒ½ä½“
def create_agent_from_config(config_type: str, **kwargs) -> BaseSQLAnalyzer:
    """ä»é…ç½®åˆ›å»ºæ™ºèƒ½ä½“çš„å·¥å‚å‡½æ•°.
    
    Args:
        config_type: é…ç½®ç±»å‹ï¼Œ"openai" æˆ– "ollama"
        **kwargs: ä¼ é€’ç»™å…·ä½“æ™ºèƒ½ä½“çš„é¢å¤–å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„æ™ºèƒ½ä½“å®ä¾‹
        
    Raises:
        ValueError: å¦‚æœé…ç½®ç±»å‹ä¸æ”¯æŒæˆ–é…ç½®æ— æ•ˆ
    """
    from .config import load_config_from_env
    
    config = load_config_from_env()
    
    if config_type == "openai":
        if config["openai"] is None:
            raise ValueError("OpenAI é…ç½®æ— æ•ˆï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
        return SQLAnalyzerAgent.from_config(config["openai"], **kwargs)
    elif config_type == "ollama":
        if config["ollama"] is None:
            raise ValueError("Ollama é…ç½®æ— æ•ˆï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡")
        return OllamaAgent.from_config(config["ollama"], **kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®ç±»å‹: {config_type}") 