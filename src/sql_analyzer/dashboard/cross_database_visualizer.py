"""è·¨æ•°æ®åº“ä¾èµ–å…³ç³»å¯è§†åŒ–ç»„ä»¶."""

import json
import logging
from datetime import datetime
from typing import Any, Dict, List, Optional

from ..database.cross_database_analyzer import CrossDatabaseAnalyzer, CrossDatabaseDependency
from .dashboard_components import DashboardComponent
from .models import ComponentConfig

logger = logging.getLogger(__name__)


class CrossDatabaseVisualizationComponent(DashboardComponent):
    """è·¨æ•°æ®åº“ä¾èµ–å…³ç³»å¯è§†åŒ–ç»„ä»¶."""
    
    def __init__(self, config: ComponentConfig, cross_db_analyzer: CrossDatabaseAnalyzer):
        super().__init__(config)
        self.cross_db_analyzer = cross_db_analyzer
        self.cached_data = None
        self.last_update = None
    
    async def get_data(self, force_refresh: bool = False) -> Dict[str, Any]:
        """è·å–å¯è§†åŒ–æ•°æ®."""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å­˜
            if (not force_refresh and self.cached_data and self.last_update and 
                (datetime.now() - self.last_update).seconds < self.config.refresh_interval):
                return self.cached_data
            
            # è·å–ä¾èµ–å…³ç³»å¯è§†åŒ–æ•°æ®
            visualization_data = await self.cross_db_analyzer.visualize_database_dependencies()
            
            # å¢å¼ºå¯è§†åŒ–æ•°æ®
            enhanced_data = await self._enhance_visualization_data(visualization_data)
            
            # æ„å»ºç»„ä»¶æ•°æ®
            component_data = {
                "component_id": self.config.component_id,
                "component_type": self.config.component_type,
                "title": self.config.title,
                "data": enhanced_data,
                "layout": self._generate_layout_config(),
                "interactions": self._generate_interaction_config(),
                "last_update": datetime.now().isoformat()
            }
            
            # ç¼“å­˜æ•°æ®
            self.cached_data = component_data
            self.last_update = datetime.now()
            
            return component_data
            
        except Exception as e:
            logger.error(f"è·å–è·¨æ•°æ®åº“å¯è§†åŒ–æ•°æ®å¤±è´¥: {e}")
            return {
                "component_id": self.config.component_id,
                "error": str(e)
            }
    
    async def _enhance_visualization_data(self, visualization_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºå¯è§†åŒ–æ•°æ®."""
        enhanced_data = visualization_data.copy()
        
        # å¢å¼ºèŠ‚ç‚¹æ•°æ®
        for node in enhanced_data.get("nodes", []):
            node.update({
                "size": self._calculate_node_size(node),
                "color": self._get_node_color(node),
                "icon": self._get_database_icon(node.get("type")),
                "tooltip": self._generate_node_tooltip(node)
            })
        
        # å¢å¼ºè¾¹æ•°æ®
        for edge in enhanced_data.get("edges", []):
            edge.update({
                "width": self._calculate_edge_width(edge),
                "color": self._get_edge_color(edge),
                "style": self._get_edge_style(edge),
                "tooltip": self._generate_edge_tooltip(edge)
            })
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        enhanced_data["statistics"] = await self._calculate_statistics(enhanced_data)
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        enhanced_data["performance_metrics"] = await self._collect_performance_metrics(enhanced_data)
        
        return enhanced_data
    
    def _calculate_node_size(self, node: Dict[str, Any]) -> int:
        """è®¡ç®—èŠ‚ç‚¹å¤§å°."""
        base_size = 30
        
        # æ ¹æ®æ•°æ®åº“çŠ¶æ€è°ƒæ•´å¤§å°
        if node.get("status") == "healthy":
            return base_size + 10
        elif node.get("status") == "degraded":
            return base_size + 5
        else:
            return base_size
    
    def _get_node_color(self, node: Dict[str, Any]) -> str:
        """è·å–èŠ‚ç‚¹é¢œè‰²."""
        status = node.get("status", "unknown")
        
        color_map = {
            "healthy": "#4CAF50",      # ç»¿è‰²
            "degraded": "#FF9800",     # æ©™è‰²
            "unhealthy": "#F44336",    # çº¢è‰²
            "unknown": "#9E9E9E"       # ç°è‰²
        }
        
        return color_map.get(status, "#9E9E9E")
    
    def _get_database_icon(self, db_type: str) -> str:
        """è·å–æ•°æ®åº“å›¾æ ‡."""
        icon_map = {
            "mysql": "ğŸ¬",
            "postgresql": "ğŸ˜",
            "oracle": "ğŸ”¶",
            "sqlserver": "ğŸ¢",
            "sqlite": "ğŸ“",
            "mongodb": "ğŸƒ",
            "redis": "ğŸ”´"
        }
        
        return icon_map.get(db_type, "ğŸ’¾")
    
    def _generate_node_tooltip(self, node: Dict[str, Any]) -> str:
        """ç”ŸæˆèŠ‚ç‚¹æç¤ºä¿¡æ¯."""
        return f"""
        æ•°æ®åº“: {node.get('label', 'Unknown')}
        ç±»å‹: {node.get('type', 'Unknown')}
        ä¸»æœº: {node.get('host', 'Unknown')}:{node.get('port', 'Unknown')}
        çŠ¶æ€: {node.get('status', 'Unknown')}
        """
    
    def _calculate_edge_width(self, edge: Dict[str, Any]) -> int:
        """è®¡ç®—è¾¹å®½åº¦."""
        strength = edge.get("strength", 0.0)
        frequency = edge.get("frequency", 0)
        
        # åŸºäºå¼ºåº¦å’Œé¢‘ç‡è®¡ç®—å®½åº¦
        base_width = 2
        strength_factor = int(strength * 5)
        frequency_factor = min(int(frequency / 10), 5)
        
        return base_width + strength_factor + frequency_factor
    
    def _get_edge_color(self, edge: Dict[str, Any]) -> str:
        """è·å–è¾¹é¢œè‰²."""
        performance_impact = edge.get("performance_impact", 0.0)
        
        if performance_impact > 0.7:
            return "#F44336"  # çº¢è‰² - é«˜å½±å“
        elif performance_impact > 0.4:
            return "#FF9800"  # æ©™è‰² - ä¸­ç­‰å½±å“
        else:
            return "#4CAF50"  # ç»¿è‰² - ä½å½±å“
    
    def _get_edge_style(self, edge: Dict[str, Any]) -> str:
        """è·å–è¾¹æ ·å¼."""
        dependency_type = edge.get("type", "")
        
        style_map = {
            "foreign_key": "solid",
            "view_dependency": "dashed",
            "stored_procedure": "dotted",
            "data_flow": "solid"
        }
        
        return style_map.get(dependency_type, "solid")
    
    def _generate_edge_tooltip(self, edge: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¾¹æç¤ºä¿¡æ¯."""
        return f"""
        ä¾èµ–ç±»å‹: {edge.get('type', 'Unknown')}
        å¼ºåº¦: {edge.get('strength', 0.0):.2f}
        é¢‘ç‡: {edge.get('frequency', 0)}
        æ€§èƒ½å½±å“: {edge.get('performance_impact', 0.0):.2f}
        æè¿°: {edge.get('description', 'No description')}
        """
    
    async def _calculate_statistics(self, visualization_data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯."""
        nodes = visualization_data.get("nodes", [])
        edges = visualization_data.get("edges", [])
        
        # æ•°æ®åº“ç±»å‹ç»Ÿè®¡
        db_types = {}
        for node in nodes:
            db_type = node.get("type", "unknown")
            db_types[db_type] = db_types.get(db_type, 0) + 1
        
        # çŠ¶æ€ç»Ÿè®¡
        status_stats = {}
        for node in nodes:
            status = node.get("status", "unknown")
            status_stats[status] = status_stats.get(status, 0) + 1
        
        # ä¾èµ–ç±»å‹ç»Ÿè®¡
        dependency_types = {}
        for edge in edges:
            dep_type = edge.get("type", "unknown")
            dependency_types[dep_type] = dependency_types.get(dep_type, 0) + 1
        
        # æ€§èƒ½å½±å“ç»Ÿè®¡
        high_impact_deps = len([e for e in edges if e.get("performance_impact", 0) > 0.7])
        medium_impact_deps = len([e for e in edges if 0.4 < e.get("performance_impact", 0) <= 0.7])
        low_impact_deps = len([e for e in edges if e.get("performance_impact", 0) <= 0.4])
        
        return {
            "total_databases": len(nodes),
            "total_dependencies": len(edges),
            "database_types": db_types,
            "status_distribution": status_stats,
            "dependency_types": dependency_types,
            "performance_impact_distribution": {
                "high": high_impact_deps,
                "medium": medium_impact_deps,
                "low": low_impact_deps
            }
        }
    
    async def _collect_performance_metrics(self, visualization_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡."""
        nodes = visualization_data.get("nodes", [])
        edges = visualization_data.get("edges", [])
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        total_strength = sum(edge.get("strength", 0) for edge in edges)
        avg_strength = total_strength / len(edges) if edges else 0
        
        total_frequency = sum(edge.get("frequency", 0) for edge in edges)
        avg_frequency = total_frequency / len(edges) if edges else 0
        
        total_impact = sum(edge.get("performance_impact", 0) for edge in edges)
        avg_impact = total_impact / len(edges) if edges else 0
        
        # è¯†åˆ«å…³é”®è·¯å¾„
        critical_paths = self._identify_critical_paths(nodes, edges)
        
        return {
            "average_dependency_strength": avg_strength,
            "average_access_frequency": avg_frequency,
            "average_performance_impact": avg_impact,
            "critical_paths": critical_paths,
            "health_score": self._calculate_overall_health_score(nodes)
        }
    
    def _identify_critical_paths(self, nodes: List[Dict[str, Any]], edges: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®è·¯å¾„."""
        critical_paths = []
        
        # æ‰¾å‡ºé«˜å½±å“çš„ä¾èµ–é“¾
        high_impact_edges = [e for e in edges if e.get("performance_impact", 0) > 0.7]
        
        for edge in high_impact_edges:
            path = {
                "source": edge.get("source"),
                "target": edge.get("target"),
                "impact": edge.get("performance_impact", 0),
                "frequency": edge.get("frequency", 0),
                "risk_level": "high" if edge.get("performance_impact", 0) > 0.8 else "medium"
            }
            critical_paths.append(path)
        
        return critical_paths
    
    def _calculate_overall_health_score(self, nodes: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•´ä½“å¥åº·è¯„åˆ†."""
        if not nodes:
            return 0.0
        
        healthy_count = len([n for n in nodes if n.get("status") == "healthy"])
        degraded_count = len([n for n in nodes if n.get("status") == "degraded"])
        unhealthy_count = len([n for n in nodes if n.get("status") == "unhealthy"])
        
        # åŠ æƒè®¡ç®—å¥åº·è¯„åˆ†
        health_score = (healthy_count * 1.0 + degraded_count * 0.5 + unhealthy_count * 0.0) / len(nodes)
        
        return health_score
    
    def _generate_layout_config(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¸ƒå±€é…ç½®."""
        return {
            "algorithm": "force-directed",
            "iterations": 100,
            "node_repulsion": 1000,
            "edge_attraction": 0.1,
            "gravity": 0.01,
            "enable_zoom": True,
            "enable_pan": True,
            "enable_drag": True,
            "show_labels": True,
            "label_threshold": 0.5
        }
    
    def _generate_interaction_config(self) -> Dict[str, Any]:
        """ç”Ÿæˆäº¤äº’é…ç½®."""
        return {
            "node_click": {
                "action": "show_details",
                "panel": "node_details"
            },
            "edge_click": {
                "action": "show_dependency_details",
                "panel": "dependency_details"
            },
            "node_hover": {
                "action": "highlight_connections",
                "show_tooltip": True
            },
            "edge_hover": {
                "action": "highlight_path",
                "show_tooltip": True
            },
            "double_click": {
                "action": "focus_subgraph"
            }
        }


class CrossDatabasePerformanceComponent(DashboardComponent):
    """è·¨æ•°æ®åº“æ€§èƒ½ç›‘æ§ç»„ä»¶."""
    
    def __init__(self, config: ComponentConfig, cross_db_analyzer: CrossDatabaseAnalyzer):
        super().__init__(config)
        self.cross_db_analyzer = cross_db_analyzer
        self.cached_data = None
        self.last_update = None
    
    async def get_data(self, force_refresh: bool = False) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç›‘æ§æ•°æ®."""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if (not force_refresh and self.cached_data and self.last_update and 
                (datetime.now() - self.last_update).seconds < self.config.refresh_interval):
                return self.cached_data
            
            # è·å–è·¨æ•°æ®åº“äº‹åŠ¡ç›‘æ§æ•°æ®
            monitoring_data = await self.cross_db_analyzer.monitor_cross_database_transactions()
            
            # æ„å»ºç»„ä»¶æ•°æ®
            component_data = {
                "component_id": self.config.component_id,
                "component_type": self.config.component_type,
                "title": self.config.title,
                "data": {
                    "transaction_metrics": monitoring_data.get("performance_metrics", {}),
                    "active_transactions": len(monitoring_data.get("active_transactions", [])),
                    "alerts": monitoring_data.get("alerts", []),
                    "recommendations": monitoring_data.get("recommendations", []),
                    "charts": await self._generate_performance_charts(monitoring_data),
                    "summary": await self._generate_performance_summary(monitoring_data)
                },
                "last_update": datetime.now().isoformat()
            }
            
            # ç¼“å­˜æ•°æ®
            self.cached_data = component_data
            self.last_update = datetime.now()
            
            return component_data
            
        except Exception as e:
            logger.error(f"è·å–è·¨æ•°æ®åº“æ€§èƒ½ç›‘æ§æ•°æ®å¤±è´¥: {e}")
            return {
                "component_id": self.config.component_id,
                "error": str(e)
            }
    
    async def _generate_performance_charts(self, monitoring_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨æ•°æ®."""
        charts = {}
        
        # äº‹åŠ¡æ•°é‡è¶‹åŠ¿å›¾
        transaction_metrics = monitoring_data.get("performance_metrics", {})
        
        charts["transaction_count"] = {
            "type": "line",
            "title": "æ´»è·ƒäº‹åŠ¡æ•°è¶‹åŠ¿",
            "data": [
                {
                    "database_id": db_id,
                    "value": metrics.get("active_transactions", 0),
                    "timestamp": datetime.now().isoformat()
                }
                for db_id, metrics in transaction_metrics.items()
            ]
        }
        
        # å¹³å‡äº‹åŠ¡æ—¶é—´å›¾
        charts["transaction_time"] = {
            "type": "bar",
            "title": "å¹³å‡äº‹åŠ¡æ‰§è¡Œæ—¶é—´",
            "data": [
                {
                    "database_id": db_id,
                    "value": metrics.get("avg_transaction_time", 0),
                    "unit": "ms"
                }
                for db_id, metrics in transaction_metrics.items()
            ]
        }
        
        # é”ç­‰å¾…å’Œæ­»é”ç»Ÿè®¡
        charts["lock_statistics"] = {
            "type": "stacked_bar",
            "title": "é”ç­‰å¾…å’Œæ­»é”ç»Ÿè®¡",
            "data": [
                {
                    "database_id": db_id,
                    "lock_waits": metrics.get("lock_waits", 0),
                    "deadlocks": metrics.get("deadlocks", 0)
                }
                for db_id, metrics in transaction_metrics.items()
            ]
        }
        
        return charts
    
    async def _generate_performance_summary(self, monitoring_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦."""
        transaction_metrics = monitoring_data.get("performance_metrics", {})
        alerts = monitoring_data.get("alerts", [])
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_active_transactions = sum(
            metrics.get("active_transactions", 0) 
            for metrics in transaction_metrics.values()
        )
        
        avg_transaction_time = sum(
            metrics.get("avg_transaction_time", 0) 
            for metrics in transaction_metrics.values()
        ) / len(transaction_metrics) if transaction_metrics else 0
        
        total_lock_waits = sum(
            metrics.get("lock_waits", 0) 
            for metrics in transaction_metrics.values()
        )
        
        total_deadlocks = sum(
            metrics.get("deadlocks", 0) 
            for metrics in transaction_metrics.values()
        )
        
        # å‘Šè­¦ç»Ÿè®¡
        alert_counts = {}
        for alert in alerts:
            severity = alert.get("severity", "info")
            alert_counts[severity] = alert_counts.get(severity, 0) + 1
        
        return {
            "total_active_transactions": total_active_transactions,
            "average_transaction_time": avg_transaction_time,
            "total_lock_waits": total_lock_waits,
            "total_deadlocks": total_deadlocks,
            "alert_counts": alert_counts,
            "monitored_databases": len(transaction_metrics),
            "health_status": "healthy" if not alert_counts.get("error", 0) else "degraded"
        }


class CrossDatabaseQueryAnalysisComponent(DashboardComponent):
    """è·¨æ•°æ®åº“æŸ¥è¯¢åˆ†æç»„ä»¶."""
    
    def __init__(self, config: ComponentConfig, cross_db_analyzer: CrossDatabaseAnalyzer):
        super().__init__(config)
        self.cross_db_analyzer = cross_db_analyzer
        self.cached_data = None
        self.last_update = None
    
    async def get_data(self, force_refresh: bool = False) -> Dict[str, Any]:
        """è·å–æŸ¥è¯¢åˆ†ææ•°æ®."""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if (not force_refresh and self.cached_data and self.last_update and 
                (datetime.now() - self.last_update).seconds < self.config.refresh_interval):
                return self.cached_data
            
            # è·å–æŸ¥è¯¢ç¼“å­˜ä¸­çš„åˆ†æç»“æœ
            query_analyses = list(self.cross_db_analyzer.query_cache.values())
            
            # æ„å»ºç»„ä»¶æ•°æ®
            component_data = {
                "component_id": self.config.component_id,
                "component_type": self.config.component_type,
                "title": self.config.title,
                "data": {
                    "query_summary": await self._generate_query_summary(query_analyses),
                    "performance_distribution": await self._generate_performance_distribution(query_analyses),
                    "optimization_suggestions": await self._aggregate_optimization_suggestions(query_analyses),
                    "cost_analysis": await self._generate_cost_analysis(query_analyses),
                    "recent_queries": await self._get_recent_queries(query_analyses)
                },
                "last_update": datetime.now().isoformat()
            }
            
            # ç¼“å­˜æ•°æ®
            self.cached_data = component_data
            self.last_update = datetime.now()
            
            return component_data
            
        except Exception as e:
            logger.error(f"è·å–è·¨æ•°æ®åº“æŸ¥è¯¢åˆ†ææ•°æ®å¤±è´¥: {e}")
            return {
                "component_id": self.config.component_id,
                "error": str(e)
            }
    
    async def _generate_query_summary(self, query_analyses: List) -> Dict[str, Any]:
        """ç”ŸæˆæŸ¥è¯¢æ‘˜è¦."""
        if not query_analyses:
            return {"total_queries": 0}
        
        # æŸ¥è¯¢ç±»å‹ç»Ÿè®¡
        query_types = {}
        for query in query_analyses:
            query_type = query.query_type.value
            query_types[query_type] = query_types.get(query_type, 0) + 1
        
        # æ¶‰åŠæ•°æ®åº“æ•°é‡ç»Ÿè®¡
        db_count_distribution = {}
        for query in query_analyses:
            db_count = len(query.involved_databases)
            db_count_distribution[str(db_count)] = db_count_distribution.get(str(db_count), 0) + 1
        
        # å¹³å‡æˆæœ¬
        avg_cost = sum(query.estimated_cost for query in query_analyses) / len(query_analyses)
        
        return {
            "total_queries": len(query_analyses),
            "query_types": query_types,
            "database_count_distribution": db_count_distribution,
            "average_estimated_cost": avg_cost
        }
    
    async def _generate_performance_distribution(self, query_analyses: List) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½åˆ†å¸ƒæ•°æ®."""
        if not query_analyses:
            return {}
        
        costs = [query.estimated_cost for query in query_analyses]
        costs.sort()
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        def percentile(data, p):
            index = int(len(data) * p / 100)
            return data[min(index, len(data) - 1)]
        
        return {
            "cost_percentiles": {
                "p50": percentile(costs, 50),
                "p75": percentile(costs, 75),
                "p90": percentile(costs, 90),
                "p95": percentile(costs, 95),
                "p99": percentile(costs, 99)
            },
            "cost_histogram": self._generate_histogram(costs, 10)
        }
    
    def _generate_histogram(self, data: List[float], bins: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç›´æ–¹å›¾æ•°æ®."""
        if not data:
            return []
        
        min_val, max_val = min(data), max(data)
        bin_width = (max_val - min_val) / bins
        
        histogram = []
        for i in range(bins):
            bin_start = min_val + i * bin_width
            bin_end = bin_start + bin_width
            count = len([x for x in data if bin_start <= x < bin_end])
            
            histogram.append({
                "range": f"{bin_start:.2f}-{bin_end:.2f}",
                "count": count
            })
        
        return histogram
    
    async def _aggregate_optimization_suggestions(self, query_analyses: List) -> Dict[str, Any]:
        """èšåˆä¼˜åŒ–å»ºè®®."""
        all_suggestions = []
        for query in query_analyses:
            all_suggestions.extend(query.optimization_suggestions)
        
        # ç»Ÿè®¡å»ºè®®é¢‘ç‡
        suggestion_counts = {}
        for suggestion in all_suggestions:
            suggestion_counts[suggestion] = suggestion_counts.get(suggestion, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        top_suggestions = sorted(
            suggestion_counts.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:10]
        
        return {
            "total_suggestions": len(all_suggestions),
            "unique_suggestions": len(suggestion_counts),
            "top_suggestions": [
                {"suggestion": suggestion, "frequency": count}
                for suggestion, count in top_suggestions
            ]
        }
    
    async def _generate_cost_analysis(self, query_analyses: List) -> Dict[str, Any]:
        """ç”Ÿæˆæˆæœ¬åˆ†æ."""
        if not query_analyses:
            return {}
        
        # æŒ‰æŸ¥è¯¢ç±»å‹åˆ†ç»„åˆ†ææˆæœ¬
        cost_by_type = {}
        for query in query_analyses:
            query_type = query.query_type.value
            if query_type not in cost_by_type:
                cost_by_type[query_type] = []
            cost_by_type[query_type].append(query.estimated_cost)
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„å¹³å‡æˆæœ¬
        avg_cost_by_type = {}
        for query_type, costs in cost_by_type.items():
            avg_cost_by_type[query_type] = sum(costs) / len(costs)
        
        return {
            "cost_by_query_type": avg_cost_by_type,
            "highest_cost_queries": sorted(
                [(query.query_id, query.estimated_cost) for query in query_analyses],
                key=lambda x: x[1],
                reverse=True
            )[:5]
        }
    
    async def _get_recent_queries(self, query_analyses: List, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„æŸ¥è¯¢."""
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰æ—¶é—´æˆ³ï¼Œè¿™é‡Œè¿”å›æœ€åå‡ ä¸ªæŸ¥è¯¢
        recent_queries = query_analyses[-limit:] if len(query_analyses) > limit else query_analyses
        
        return [
            {
                "query_id": query.query_id,
                "query_type": query.query_type.value,
                "involved_databases": len(query.involved_databases),
                "estimated_cost": query.estimated_cost,
                "optimization_count": len(query.optimization_suggestions)
            }
            for query in recent_queries
        ]